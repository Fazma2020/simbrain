<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Actor Critic Simulation</title>
</head>

<body>
    <!--  TODO: Add exercizes where cheeses are added, rewards moved around, etc. -->
    <!--  TODO: Once value representations are added to the grid cells describe these -->
    <!--  TODO: Add background information.  When an action leads to reward reinforce the last-state > value link since that led to the reward, and reinforce the last-state > action reward since that action led to reward -->
    <!--  TODO: Need anatomy model to supplement, with GPe, etc. (Cerebellum kind of has this)   In this model, sensory neurons are like sensory or hippocampus > cortical state > striatum, actions are kind of like GPi>thalamus>cortex, and td error is like substantia nigra pars compacta, reward is like VTA, but it's all very abstracted.  -->
    <!--  TODO: Discuss ways the simulation can be modified directly  -->
    <h1>Actor Critic model</h1>
    <p>Based on Richard Sutton (1996), Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. Simbrain implementation by Jeff Yoshimi and Jonathon Vickrey.</p>
    <h2>Getting started</h2>
    <p>A model which learns the location of rewarding stimuli. Do a few runs through 5 trials using the "run" button on the control panel. Using default values, the rat should figure out how to get the cheese.</p>
    <h2>Parameters and what they mean</h2>
    <p><b>Epsilon</b>: Probability of taking a random action. 0 for no random actions; 1 for all random actions. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2645553/">Doya, 2007</a> suggests this may be related to noradernaline (or Norepinephrine) which regulates overall arousal (it decreases when asleep, rises when awake, etc.) </p>
    <!--  TODO: Discuss exploitation / exploration  -->
    <p><b>Learning rate</b>:How much weights are updated at each time step. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2645553/">Doya, 2007</a> suggests this may be related to acetlycholine in the brain, which regulates some forms of plasticity.</p>
    <p><b>Discount Factor (gamma)</b>: Determines how "future oriented" the agent is. Range is 0-1. For values closer to 0 the agent is more focused on immediate rewards, it is "short-sighted" or impulsive. For values closer to 1 the agent is more focused on distant rewards. The agent will not learn when gamma is 0 because it only cares about immediate reward and never learns to attach values to states <i>leading to</i> reward.</p>
    <p>Higher values of gamma produce better results in this model. The agent thinks ahead, and takes actions that will lead in the long run to cheese; the agent attaches value to states associated with other states that are associated via a chain of actions to the cheese.</p>
    <p>Tanaka et. al. have related the discount factor to serotonin in the brain:
        <blockquote>
            to elucidate the role of serotonin in the evaluation of delayed rewards, we performed a functional brain imaging experiment in which subjects chose small-immediate or large-delayed liquid rewards under dietary regulation of tryptophan, a precursor of serotonin. A model-based analysis revealed that <i>the activity of the ventral part of the striatum was correlated with reward prediction at shorter time scales, and this correlated activity was stronger at low serotonin levels</i>. By contrast, <i>the activity of the dorsal part of the striatum was correlated with reward prediction at longer time scales, and this correlated activity was stronger at high serotonin levels.</i> (<a href="https://www.ncbi.nlm.nih.gov/pubmed/18091999">Tanaka et. al, 2007</a>, our emphasis).</blockquote>
    </p>
    <h2>Time Series</h2>
    <p>The <b>red time series</b> shows the activation of the value neuron. This means a tile either has reward or (together with correct actions) leads to reward.</p>
    <p>The <b>blue time series</b> shows the activation of the td error neuron. When something unexpected happens this goes up and the network learns.</p>
</body>

</html>
